# OriginalTransformer
My implementation of the original transformer in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762).

I'm drawing heavily on [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) blogpost.
